#!/usr/bin/env python3
"""
Test script for Canis AI Advanced Features
Tests real-time inference, model versioning, async tasks, and monitoring
"""

import requests
import json
import time
import sys
import os

# Add the current directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

BASE_URL = "http://127.0.0.1:8000/api/v1"

def test_health_check():
    """Test health check endpoint"""
    print("ğŸ” Testing Health Check...")
    try:
        response = requests.get(f"{BASE_URL}/healthcheck/")
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Health Check: {data['status']}")
            print(f"   Uptime: {data['system']['uptime_formatted']}")
            print(f"   CPU: {data['system']['cpu']['usage_percent']}%")
            print(f"   Memory: {data['system']['memory']['percent']}%")
            return True
        else:
            print(f"âŒ Health Check failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Health Check error: {str(e)}")
        return False

def test_model_info():
    """Test model info endpoint"""
    print("\nğŸ” Testing Model Info...")
    try:
        response = requests.get(f"{BASE_URL}/model-info/")
        if response.status_code == 200:
            data = response.json()
            if "error" in data:
                print(f"âš ï¸  Model Info: {data['error']}")
            else:
                print(f"âœ… Model Info: {data['model_type']}")
                print(f"   Task Type: {data['task_type']}")
                print(f"   Target Column: {data['target_column']}")
            return True
        else:
            print(f"âŒ Model Info failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Model Info error: {str(e)}")
        return False

def test_prediction():
    """Test real-time prediction"""
    print("\nğŸ” Testing Real-time Prediction...")
    try:
        # Sample data for prediction
        sample_data = {
            "feature1": 1.5,
            "feature2": 2.3,
            "feature3": 0.8,
            "feature4": 1.2
        }
        
        response = requests.post(f"{BASE_URL}/predict/", json=sample_data)
        if response.status_code == 200:
            data = response.json()
            if "error" in data:
                print(f"âš ï¸  Prediction: {data['error']}")
            else:
                print(f"âœ… Prediction successful!")
                print(f"   Predictions: {data['predictions']}")
                if 'probabilities' in data:
                    print(f"   Probabilities: {data['probabilities']}")
            return True
        else:
            print(f"âŒ Prediction failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Prediction error: {str(e)}")
        return False

def test_model_versioning():
    """Test model versioning system"""
    print("\nğŸ” Testing Model Versioning...")
    try:
        # Save model version
        print("   Saving model version...")
        response = requests.post(f"{BASE_URL}/save-model-version/", params={
            "model_name": "test_model",
            "description": "Test model for versioning",
            "tags": "test,regression"
        })
        
        if response.status_code == 200:
            data = response.json()
            if "error" in data:
                print(f"âš ï¸  Save Model: {data['error']}")
                return False
            else:
                print(f"âœ… Model saved: {data['model_name']}_{data['version']}")
                
                # List models
                print("   Listing models...")
                response = requests.get(f"{BASE_URL}/list-models/")
                if response.status_code == 200:
                    data = response.json()
                    if "error" in data:
                        print(f"âš ï¸  List Models: {data['error']}")
                    else:
                        print(f"âœ… Found {data['total_models']} models")
                        for model in data['models'][:3]:  # Show first 3
                            print(f"     - {model['model_name']}_{model['version']} ({model['task_type']})")
                return True
        else:
            print(f"âŒ Save Model failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Model Versioning error: {str(e)}")
        return False

def test_async_tasks():
    """Test async task queue"""
    print("\nğŸ” Testing Async Task Queue...")
    try:
        # Start async benchmark
        print("   Starting async benchmark...")
        response = requests.post(f"{BASE_URL}/async-benchmark/")
        
        if response.status_code == 200:
            data = response.json()
            if "error" in data:
                print(f"âš ï¸  Async Benchmark: {data['error']}")
                return False
            else:
                task_id = data['task_id']
                print(f"âœ… Benchmark task started: {task_id}")
                
                # Check task status
                print("   Checking task status...")
                for i in range(5):  # Check 5 times
                    time.sleep(2)
                    response = requests.get(f"{BASE_URL}/task-status/", params={"task_id": task_id})
                    if response.status_code == 200:
                        task_data = response.json()
                        if "error" not in task_data:
                            status = task_data['status']
                            progress = task_data.get('progress', 0)
                            print(f"     Status: {status} (Progress: {progress}%)")
                            
                            if status in ['completed', 'failed', 'cancelled']:
                                break
                
                # Get all tasks
                print("   Getting all tasks...")
                response = requests.get(f"{BASE_URL}/all-tasks/")
                if response.status_code == 200:
                    data = response.json()
                    if "error" not in data:
                        print(f"âœ… Total tasks: {len(data)}")
                return True
        else:
            print(f"âŒ Async Benchmark failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Async Tasks error: {str(e)}")
        return False

def test_benchmark_endpoints():
    """Test benchmark endpoints"""
    print("\nğŸ” Testing Benchmark Endpoints...")
    try:
        # Test benchmark models
        print("   Running benchmark...")
        response = requests.post(f"{BASE_URL}/benchmark-models/")
        
        if response.status_code == 200:
            data = response.json()
            if "error" in data:
                print(f"âš ï¸  Benchmark: {data['error']}")
                return False
            else:
                print(f"âœ… Benchmark completed!")
                print(f"   Task Type: {data['task_type']}")
                print(f"   Best Score: {data['best_score']}")
                print(f"   Models Tested: {len(data['all_results'])}")
                
                # Test benchmark summary
                print("   Getting benchmark summary...")
                response = requests.get(f"{BASE_URL}/benchmark-summary/")
                if response.status_code == 200:
                    summary_data = response.json()
                    if "error" not in summary_data:
                        print(f"âœ… Summary: {summary_data['total_models_tested']} models tested")
                return True
        else:
            print(f"âŒ Benchmark failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Benchmark error: {str(e)}")
        return False

def main():
    """Run all tests"""
    print("ğŸš€ Testing Canis AI Advanced Features")
    print("=" * 50)
    
    tests = [
        test_health_check,
        test_model_info,
        test_prediction,
        test_model_versioning,
        test_async_tasks,
        test_benchmark_endpoints
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"âŒ Test failed with exception: {str(e)}")
            results.append(False)
    
    print("\n" + "=" * 50)
    print("ğŸ“Š Test Results Summary")
    print("=" * 50)
    
    test_names = [
        "Health Check",
        "Model Info", 
        "Real-time Prediction",
        "Model Versioning",
        "Async Task Queue",
        "Benchmark Endpoints"
    ]
    
    passed = 0
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{i+1}. {name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ Overall: {passed}/{len(tests)} tests passed")
    
    if passed == len(tests):
        print("ğŸ‰ All tests passed! Your Canis AI Backend is working perfectly!")
    else:
        print("âš ï¸  Some tests failed. Check the server logs for details.")

if __name__ == "__main__":
    main() 